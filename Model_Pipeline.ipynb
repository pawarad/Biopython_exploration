{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numba import jit\n",
    "from biopandas.pdb import PandasPdb\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from antibody.SequenceUtils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Meiler's feature for padding along with one hot encoding based on structural properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_one_hot(res_seq_one):\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    ints = one_to_number(res_seq_one)\n",
    "    feats = aa_features()[ints]\n",
    "    onehot = to_categorical(ints, num_classes=len(aa_s))\n",
    "    return np.concatenate((onehot, feats), axis=1)\n",
    "\n",
    "aa_s = \"CSTPAGNDEQHRKMILVFYWX\"\n",
    "def one_to_number(res_str):\n",
    "    return [aa_s.index(r) for r in res_str]\n",
    "\n",
    "def aa_features():\n",
    "    # Meiler's features\n",
    "    prop1 = [[1.77, 0.13, 2.43,  1.54,  6.35, 0.17, 0.41],\n",
    "             [1.31, 0.06, 1.60, -0.04,  5.70, 0.20, 0.28],\n",
    "             [3.03, 0.11, 2.60,  0.26,  5.60, 0.21, 0.36],\n",
    "             [2.67, 0.00, 2.72,  0.72,  6.80, 0.13, 0.34],\n",
    "             [1.28, 0.05, 1.00,  0.31,  6.11, 0.42, 0.23],\n",
    "             [0.00, 0.00, 0.00,  0.00,  6.07, 0.13, 0.15],\n",
    "             [1.60, 0.13, 2.95, -0.60,  6.52, 0.21, 0.22],\n",
    "             [1.60, 0.11, 2.78, -0.77,  2.95, 0.25, 0.20],\n",
    "             [1.56, 0.15, 3.78, -0.64,  3.09, 0.42, 0.21],\n",
    "             [1.56, 0.18, 3.95, -0.22,  5.65, 0.36, 0.25],\n",
    "             [2.99, 0.23, 4.66,  0.13,  7.69, 0.27, 0.30],\n",
    "             [2.34, 0.29, 6.13, -1.01, 10.74, 0.36, 0.25],\n",
    "             [1.89, 0.22, 4.77, -0.99,  9.99, 0.32, 0.27],\n",
    "             [2.35, 0.22, 4.43,  1.23,  5.71, 0.38, 0.32],\n",
    "             [4.19, 0.19, 4.00,  1.80,  6.04, 0.30, 0.45],\n",
    "             [2.59, 0.19, 4.00,  1.70,  6.04, 0.39, 0.31],\n",
    "             [3.67, 0.14, 3.00,  1.22,  6.02, 0.27, 0.49],\n",
    "             [2.94, 0.29, 5.89,  1.79,  5.67, 0.30, 0.38],\n",
    "             [2.94, 0.30, 6.47,  0.96,  5.66, 0.25, 0.41],\n",
    "             [3.21, 0.41, 8.08,  2.25,  5.94, 0.32, 0.42],\n",
    "             [0.00, 0.00, 0.00,  0.00,  0.00, 0.00, 0.00]]\n",
    "    return np.array(prop1)\n",
    "NUM_FEATURES = len(aa_s) + 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import Model\n",
    "from keras.layers import Layer, Bidirectional, TimeDistributed, \\\n",
    "    Dense, LSTM, Masking, Input, RepeatVector, Dropout, Convolution1D, \\\n",
    "    BatchNormalization, Activation\n",
    "from keras.layers.merge import concatenate, add\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskingByLambda(Layer):\n",
    "    def __init__(self, func, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.mask_func = func\n",
    "        super(MaskingByLambda, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return self.mask_func(input, input_mask)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        exd_mask = K.expand_dims(self.mask_func(x, mask), axis=-1)\n",
    "        return x * K.cast(exd_mask, K.floatx())\n",
    "\n",
    "def mask_by_input(tensor):\n",
    "        return lambda input, mask: tensor\n",
    "\n",
    "\n",
    "# 1D convolution that supports masking by retaining the mask of the input\n",
    "class MaskedConvolution1D(Convolution1D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        assert kwargs['padding'] == 'same' # Only makes sense for 'same'\n",
    "        super(MaskedConvolution1D, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return input_mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        assert mask is not None\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        x = super(MaskedConvolution1D, self).call(x)\n",
    "        return x * K.cast(mask, K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_ab_seq_model(max_cdr_len):\n",
    "    input_ab = Input(shape=(max_cdr_len, NUM_FEATURES))\n",
    "    label_mask = Input(shape=(max_cdr_len,))\n",
    "\n",
    "    seq = MaskingByLambda(mask_by_input(label_mask))(input_ab)\n",
    "    loc_fts = MaskedConvolution1D(28, 3, padding='same', activation='elu',\n",
    "                                  kernel_regularizer=l2(0.01))(seq)\n",
    "\n",
    "    res_fts = add([seq, loc_fts])\n",
    "\n",
    "    glb_fts = Bidirectional(LSTM(256, dropout=0.15, recurrent_dropout=0.2,\n",
    "                                 return_sequences=True),\n",
    "                            merge_mode='concat')(res_fts)\n",
    "\n",
    "    fts = Dropout(0.3)(glb_fts)\n",
    "    probs = TimeDistributed(Dense(1, activation='sigmoid',\n",
    "                                  kernel_regularizer=l2(0.01)))(fts)\n",
    "    return input_ab, label_mask, res_fts, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_seq_model(max_cdr_len):\n",
    "    input_ab, label_mask, _, probs = base_ab_seq_model(max_cdr_len)\n",
    "    model = Model(inputs=[input_ab, label_mask], outputs=probs)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy', false_pos, false_neg],\n",
    "                  sample_weight_mode=\"temporal\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_ab_seq_model(max_cdr_len):\n",
    "    input_ab, label_mask, loc_fts, probs = base_ab_seq_model(max_cdr_len)\n",
    "    model = Model(inputs=[input_ab, label_mask], outputs=[probs, loc_fts])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_neg(y_true, y_pred):\n",
    "    return K.squeeze(K.clip(y_true - K.round(y_pred), 0.0, 1.0), axis=-1)\n",
    "\n",
    "\n",
    "def false_pos(y_true, y_pred):\n",
    "    return K.squeeze(K.clip(K.round(y_pred) - y_true, 0.0, 1.0), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_with_lengths(matrix, lengths):\n",
    "    seqs = []\n",
    "    for i, example in enumerate(matrix):\n",
    "        seq = example[:lengths[i]]\n",
    "        seqs.append(seq)\n",
    "    return np.concatenate(seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youden_j_stat(fpr, tpr, thresholds):\n",
    "    j_ordered = sorted(zip(tpr - fpr, thresholds))\n",
    "    return j_ordered[-1][1]\n",
    "\n",
    "\n",
    "def compute_classifier_metrics(labels, probs):\n",
    "    matrices = []\n",
    "    aucs = []\n",
    "    mcorrs = []\n",
    "    jstats = []\n",
    "\n",
    "    for l, p in zip(labels, probs):\n",
    "        jstats.append(youden_j_stat(*roc_curve(l, p)))\n",
    "\n",
    "    jstat_scores = np.array(jstats)\n",
    "    jstat = np.mean(jstat_scores)\n",
    "    jstat_err = 2 * np.std(jstat_scores)\n",
    "\n",
    "    threshold = jstat\n",
    "\n",
    "    print(\"Youden's J statistic = {} +/- {}. Using it as threshold.\".format(jstat, jstat_err))\n",
    "\n",
    "    for l, p in zip(labels, probs):\n",
    "        aucs.append(roc_auc_score(l, p))\n",
    "        l_pred = (p > threshold).astype(int)\n",
    "        matrices.append(confusion_matrix(l, l_pred))\n",
    "        mcorrs.append(matthews_corrcoef(l, l_pred))\n",
    "\n",
    "    matrices = np.stack(matrices)\n",
    "    mean_conf = np.mean(matrices, axis=0)\n",
    "    errs_conf = 2 * np.std(matrices, axis=0)\n",
    "\n",
    "    tps = matrices[:, 1, 1]\n",
    "    fns = matrices[:, 1, 0]\n",
    "    fps = matrices[:, 0, 1]\n",
    "\n",
    "    recalls = tps / (tps + fns)\n",
    "    precisions = tps / (tps + fps)\n",
    "\n",
    "    rec = np.mean(recalls)\n",
    "    rec_err = 2 * np.std(recalls)\n",
    "    prec = np.mean(precisions)\n",
    "    prec_err = 2 * np.std(precisions)\n",
    "\n",
    "    fscores = 2 * precisions * recalls / (precisions + recalls)\n",
    "    fsc = np.mean(fscores)\n",
    "    fsc_err = 2 * np.std(fscores)\n",
    "\n",
    "    auc_scores = np.array(aucs)\n",
    "    auc = np.mean(auc_scores)\n",
    "    auc_err = 2 * np.std(auc_scores)\n",
    "\n",
    "    mcorr_scores = np.array(mcorrs)\n",
    "    mcorr = np.mean(mcorr_scores)\n",
    "    mcorr_err = 2 * np.std(mcorr_scores)\n",
    "\n",
    "    print(\"Mean confusion matrix and error\")\n",
    "    print(mean_conf)\n",
    "    print(errs_conf)\n",
    "\n",
    "    print(\"Recall = {} +/- {}\".format(rec, rec_err))\n",
    "    print(\"Precision = {} +/- {}\".format(prec, prec_err))\n",
    "    print(\"F-score = {} +/- {}\".format(fsc, fsc_err))\n",
    "    print(\"ROC AUC = {} +/- {}\".format(auc, auc_err))\n",
    "    print(\"MCC = {} +/- {}\".format(mcorr, mcorr_err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_ids_to_selection_mask(idx, num_structures):\n",
    "    mask = np.zeros((num_structures * 6, ), dtype=np.bool)\n",
    "    offset = idx * 6\n",
    "    for i in range(6):\n",
    "        mask[offset + i] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv_eval(model_func, output_file,\n",
    "                  weights_template, seed=0):\n",
    "#     cdrs, lbls, masks = dataset[\"cdrs\"], dataset[\"lbls\"], dataset[\"masks\"]\n",
    "    kf = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\n",
    "    all_lbls = []\n",
    "    all_probs = []\n",
    "    all_masks = []\n",
    "\n",
    "    num_structures = int(len(cdrs) / 6)\n",
    "    for i, (train_ids, test_ids) in enumerate(kf.split(np.arange(num_structures))):\n",
    "        print(\"Fold: \", i + 1)\n",
    "\n",
    "        train_idx = structure_ids_to_selection_mask(train_ids, num_structures)\n",
    "        test_idx = structure_ids_to_selection_mask(test_ids, num_structures)\n",
    "\n",
    "        cdrs_train, lbls_train, mask_train = cdrs[train_idx], lbls[train_idx], masks[train_idx]\n",
    "        cdrs_test, lbls_test, mask_test = cdrs[test_idx], lbls[test_idx], masks[test_idx]\n",
    "\n",
    "        example_weight = np.squeeze((lbls_train * 1.5 + 1) * mask_train)\n",
    "        test_ex_weight = np.squeeze((lbls_test * 1.5 + 1) * mask_test)\n",
    "        model = ab_seq_model(max_cdr_len)\n",
    "\n",
    "        rate_schedule = lambda e: 0.001 if e >= 10 else 0.01\n",
    "\n",
    "        model.fit([cdrs_train, np.squeeze(mask_train)],\n",
    "                  lbls_train, batch_size=32, epochs=18,\n",
    "                  # For informational purposes about the best number of epochs\n",
    "                  # TODO: replace for proper evaluation\n",
    "                  # validation_data=([cdrs_test, np.squeeze(mask_test)],\n",
    "                  #                  lbls_test, test_ex_weight),\n",
    "                  sample_weight=example_weight,\n",
    "                  callbacks=[LearningRateScheduler(rate_schedule)])\n",
    "\n",
    "        model.save_weights(weights_template.format(i))\n",
    "\n",
    "        probs_test = model.predict([cdrs_test, np.squeeze(mask_test)])\n",
    "        all_lbls.append(lbls_test)\n",
    "        all_probs.append(probs_test)\n",
    "        all_masks.append(mask_test)\n",
    "\n",
    "    lbl_mat = np.concatenate(all_lbls)\n",
    "    prob_mat = np.concatenate(all_probs)\n",
    "    mask_mat = np.concatenate(all_masks)\n",
    "\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump((lbl_mat, prob_mat, mask_mat), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(output_folder, num_iters=10):\n",
    "#     cache_file = dataset.split(\"/\")[-1] + \".p\"\n",
    "#     dataset = open_dataset(dataset, dataset_cache=cache_file)\n",
    "    model_factory = lambda: ab_seq_model(Max_len_CDR)\n",
    "\n",
    "    makedirs(output_folder + \"/weights\", exist_ok=True)\n",
    "    iters = range(num_iters) if type(num_iters) is int else range(*num_iters)\n",
    "    for i in iters:\n",
    "        print(\"Crossvalidation run\", i+1)\n",
    "        output_file = \"{}/run-{}.p\".format(output_folder, i)\n",
    "        weights_template = output_folder + \"/weights/run-\" + \\\n",
    "                           str(i) + \"-fold-{}.h5\"\n",
    "        kfold_cv_eval(model_factory, \n",
    "                      output_file, weights_template, seed=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_crossval_results(folder, num_results,\n",
    "                          loop_filter=None, flatten_by_lengths=True):\n",
    "    class_probabilities = []\n",
    "    labels = []\n",
    "\n",
    "    for r in range(num_results):\n",
    "        result_filename = \"{}/run-{}.p\".format(folder, r)\n",
    "        with open(result_filename, \"rb\") as f:\n",
    "            lbl_mat, prob_mat, mask_mat = pickle.load(f)\n",
    "\n",
    "        # Get entries corresponding to the given loop\n",
    "        if loop_filter is not None:\n",
    "            lbl_mat = lbl_mat[loop_filter::6]\n",
    "            prob_mat = prob_mat[loop_filter::6]\n",
    "            mask_mat = mask_mat[loop_filter::6]\n",
    "\n",
    "        if not flatten_by_lengths:\n",
    "            class_probabilities.append(prob_mat)\n",
    "            labels.append(lbl_mat)\n",
    "            continue\n",
    "\n",
    "        # Discard sequence padding\n",
    "        seq_lens = np.sum(np.squeeze(mask_mat), axis=1)\n",
    "        p = flatten_with_lengths(prob_mat, seq_lens)\n",
    "        l = flatten_with_lengths(lbl_mat, seq_lens)\n",
    "\n",
    "        class_probabilities.append(p)\n",
    "        labels.append(l)\n",
    "\n",
    "    return labels, class_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(labels_test, probs_test, colours=(\"#0072CF\", \"#68ACE5\"),\n",
    "                   label=\"This method\", plot_fig=None):\n",
    "    if plot_fig is None:\n",
    "        plot_fig = plt.figure(figsize=(3.7, 3.7), dpi=400)\n",
    "    ax = plot_fig.gca()\n",
    "\n",
    "    num_runs = len(labels_test)\n",
    "    tprs = np.zeros((num_runs, 10000))\n",
    "    fprs = np.linspace(0.0, 1.0, num=10000)\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        l = labels_test[i]\n",
    "        p = probs_test[i]\n",
    "\n",
    "        fpr, tpr, _ = metrics.roc_curve(l.flatten(), p.flatten())\n",
    "\n",
    "        for j, fpr_val in enumerate(fprs):  # Inefficient, but good enough\n",
    "            for t, f in zip(tpr, fpr):\n",
    "                if f >= fpr_val:\n",
    "                    tprs[i, j] = t\n",
    "                    break\n",
    "\n",
    "    avg_tpr = np.average(tprs, axis=0)\n",
    "    err_tpr = np.std(tprs, axis=0)\n",
    "\n",
    "    ax.plot(fprs, avg_tpr, c=colours[0], label=label)\n",
    "\n",
    "    btm_err = avg_tpr - 2 * err_tpr\n",
    "    btm_err[btm_err < 0.0] = 0.0\n",
    "    top_err = avg_tpr + 2 * err_tpr\n",
    "    top_err[top_err > 1.0] = 1.0\n",
    "\n",
    "    ax.fill_between(fprs, btm_err, top_err, facecolor=colours[1])\n",
    "\n",
    "    ax.set_ylabel(\"True positive rate\")\n",
    "    ax.set_xlabel(\"False positive rate\")\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    return plot_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(labels_test, probs_test, colours=(\"#0072CF\", \"#68ACE5\"),\n",
    "                  label=\"This method\", plot_fig=None):\n",
    "    if plot_fig is None:\n",
    "        plot_fig = plt.figure(figsize=(4.5, 3.5), dpi=300)\n",
    "    ax = plot_fig.gca()\n",
    "\n",
    "    num_runs = len(labels_test)\n",
    "    precs = np.zeros((num_runs, 10000))\n",
    "    recalls = np.linspace(0.0, 1.0, num=10000)\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        l = labels_test[i]\n",
    "        p = probs_test[i]\n",
    "\n",
    "        prec, rec, _ = metrics.precision_recall_curve(l.flatten(), p.flatten())\n",
    "\n",
    "        # Maximum interpolation\n",
    "        for j in range(len(prec)):\n",
    "            prec[j] = prec[:(j+1)].max()\n",
    "\n",
    "        prec = list(reversed(prec))\n",
    "        rec = list(reversed(rec))\n",
    "\n",
    "        for j, recall in enumerate(recalls):  # Inefficient, but good enough\n",
    "            for p, r in zip(prec, rec):\n",
    "                if r >= recall:\n",
    "                    precs[i, j] = p\n",
    "                    break\n",
    "\n",
    "    avg_prec = np.average(precs, axis=0)\n",
    "    err_prec = np.std(precs, axis=0)\n",
    "\n",
    "    ax.plot(recalls, avg_prec, c=colours[0], label=label)\n",
    "\n",
    "    btm_err = avg_prec - 2 * err_prec\n",
    "    btm_err[btm_err < 0.0] = 0.0\n",
    "    top_err = avg_prec + 2 * err_prec\n",
    "    top_err[top_err > 1.0] = 1.0\n",
    "\n",
    "    ax.fill_between(recalls, btm_err, top_err, facecolor=colours[1])\n",
    "\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.legend()\n",
    "\n",
    "    print(\"Average precision: {}\".format(np.mean(avg_prec)))\n",
    "\n",
    "    return plot_fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding extra 2 residues on each side of CDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCDRLoops_padded(seq,chain,_scheme='chothia'):\n",
    "    #seq is numbered\n",
    "    if chain=='L':\n",
    "        pass\n",
    "    elif chain=='H':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('chain must be \"H\" or \"L\"')\n",
    "    from collections import deque      \n",
    "# chain = 'H'\n",
    "# _scheme='chothia'\n",
    "    toret = {}\n",
    "    for i in ['1','2','3','4']:\n",
    "        if i in ['1','2','3']:\n",
    "            loop = chain + i\n",
    "            toret[loop] = {'seq':{},'seqstr':'','length':0}\n",
    "            target = getCDRPos(loop,_scheme)\n",
    "    #         a = target[0]-2\n",
    "            for j in [1,1]:\n",
    "                target = deque(target)\n",
    "                target.appendleft(str(int(target[0])-j)) ## adding 2 seq to CDR in front\n",
    "                target.append(str(int(target[-1])+j)) ## adding 2 seq to CDR at back\n",
    "    #     \n",
    "    #         print(loop,target,target[-1])\n",
    "            for n in target:\n",
    "                if n in seq:\n",
    "                    if seq[n]!='-':\n",
    "                        toret[loop]['seq'][n] = seq[n]\n",
    "                        toret[loop]['length']+=1\n",
    "                        toret[loop]['seqstr']+=seq[n]\n",
    "\n",
    "            fw = chain + i + 'F'\n",
    "            toret[fw] = {'seq':{},'seqstr':'','length':0}\n",
    "            for n in getCDRPos(fw,_scheme):\n",
    "                if n in seq:\n",
    "                    if seq[n]!='-':\n",
    "                        toret[fw]['seq'][n] = seq[n]\n",
    "                        toret[fw]['length']+=1\n",
    "                        toret[fw]['seqstr']+=seq[n]  \n",
    "    return toret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read summary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "# with open('20200610_0818458_summary.tsv') as tsvfile:\n",
    "#     reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "#     for row in reader:\n",
    "#         df.append(row[0:5])\n",
    "#     data = pd.DataFrame(df)\n",
    "#     data.columns = data.iloc[0]\n",
    "#     sample = data[1:]\n",
    "#     sample_file = sample.drop_duplicates(['pdb'])\n",
    "#     sample_file = sample_file[sample_file['antigen_chain'] != 'NA']\n",
    "sample_file = pd.read_csv(\"/Users/apawar/Desktop/PERITIA/summary_file_cleaned.csv\")\n",
    "\"# sample_file\\n\",\n",
    "Hchain = sample_file['Hchain'].to_numpy()\n",
    "Lchain = sample_file['Lchain'].to_numpy()\n",
    "pdb = sample_file['pdb'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract only CDR seq from both heavy and light chain to calculate max length of CDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cdr_loops(pdb,pdb_path):\n",
    "    CDR = []\n",
    "    Type = []\n",
    "    pdb_name = []\n",
    "    seq_no =[]\n",
    "\n",
    "    for i in range(len(pdb)):\n",
    "        cdrs = {}\n",
    "        ppdb = PandasPdb().read_pdb(pdb_path + pdb[i]+'.pdb')\n",
    "        sequence = ppdb.amino3to1()\n",
    "        vh = ''.join(sequence[sequence.chain_id== Hchain[i]].residue_name.values)\n",
    "        vh_numbered, vh_info = numberSequence(vh)\n",
    "        Chot= getCDRLoops_padded(vh_numbered,'H','chothia')\n",
    "        cdrs.update(Chot)\n",
    "\n",
    "        vh1 = ''.join(sequence[sequence.chain_id== Lchain[i]].residue_name.values)\n",
    "        vh_numbered1, vh_info1 = numberSequence(vh1)\n",
    "        Chot1= getCDRLoops_padded(vh_numbered1,'L','chothia')\n",
    "        cdrs.update(Chot1)\n",
    "\n",
    "        for key, value in cdrs.items():\n",
    "            if \"F\" not in key:\n",
    "                k = key\n",
    "                Type.append(k)\n",
    "                seq = cdrs[k]['seqstr']\n",
    "                res_no = cdrs[k]['seq']\n",
    "                seq_no.append(list(res_no.keys()))\n",
    "                CDR.append(seq) \n",
    "                pdb_name.append(pdb[i])\n",
    "    return pdb_name,Type,seq_no, CDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the maximum length of CDR loop for padding\n",
    "def maxi_len(all_variables):\n",
    "    max_length = 0\n",
    "    for i in range(len(all_variables)):\n",
    "        length = len(all_variables[i])\n",
    "        max_length = max(max_length,length)\n",
    "    #     max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off):\n",
    "    \"\"\"\n",
    "    For every CDR get the corresponding seq and check if dist is less than cutoff\n",
    "    and substitute 1 orelse 0 \n",
    "    \n",
    "    E.g. - CDR - TCRASGNIHNYLAWY\n",
    "           Seq.no - ['22','23','24','25','26','27','28','29','30','31','32','33','34','35','36']\n",
    "           \n",
    "           Output - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    pdb_name = cdr_loop['PDB_Name'].to_numpy()\n",
    "    seq_no = cdr_loop['seq_no'].to_numpy()\n",
    "    chain_type = cdr_loop['Type'].to_numpy()\n",
    "\n",
    "    all_var = []\n",
    "    for i in range(len(pdb_name)):\n",
    "        if chain_type[i] == 'H1' or chain_type[i] == 'H2' or chain_type[i] == 'H3':\n",
    "            dist = pd.read_pickle(dist_matx_path_heavy+pdb_name[i]+\"heavy\"+\".pkl\")\n",
    "            var = []\n",
    "            for j in range(len(seq_no[i])):\n",
    "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
    "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
    "\n",
    "                if np.any(row_slc < cut_off) == True:\n",
    "                    var.append(1)\n",
    "                else:\n",
    "                    var.append(0)\n",
    "            all_var.append(var)\n",
    "\n",
    "        elif chain_type[i] == 'L1' or chain_type[i] == 'L2' or chain_type[i] == 'L3':\n",
    "            dist = pd.read_pickle(dist_matx_path_light+pdb_name[i]+\"light\"+\".pkl\")\n",
    "            var = []\n",
    "            for j in range(len(seq_no[i])):\n",
    "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
    "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
    "\n",
    "                if np.any(row_slc < cut_off) == True:\n",
    "                    var.append(1)\n",
    "                else:\n",
    "                    var.append(0)\n",
    "            all_var.append(var)\n",
    "    \n",
    "    return all_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Max_len_CDR = maxi_len(CDR)\n",
    "Max_len_CDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Executing extract_cdr_loops and storing everyting in df \n",
    "pdb_path = \"/Users/apawar/Desktop/PERITIA/cleaned_pdb_shweta/\"\n",
    "pdb_name,Type,seq_no,CDR = extract_cdr_loops(pdb,pdb_path)\n",
    "\n",
    "### Dataframe of odb,seqno and CDR\n",
    "CDR_heavy_clean_padded = pd.DataFrame(list(zip(pdb_name,Type,seq_no, CDR)), \n",
    "               columns =['PDB_Name','Type','seq_no','CDR'])\n",
    "CDR_heavy_clean_padded.to_pickle(\"./CDRs_seqno_pdb.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr_loop = pd.read_pickle(\"CDRs_seqno_pdb.pkl\")\n",
    "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
    "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
    "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
    "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
    "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
    "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
    "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
    "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\"])].index)\n",
    "cdr_loop.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dist_matx_path_light = \"/Users/apawar/Desktop/PERITIA/Light_chain_dist_clean/\"\n",
    "dist_matx_path_heavy = \"/Users/apawar/Desktop/PERITIA/Heavy_chain_dist_clean/\"\n",
    "cut_off = 3\n",
    "all_variables = get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3816, 38, 1)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Matrix of labels\n",
    "\n",
    "cont_mats = []\n",
    "\"\"\"\n",
    "Pad the variables to Max CDR len with 0\n",
    "Here max len of CDR is 38\n",
    "\n",
    "E.g - Input = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    Output = [0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[1.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.]\n",
    "\n",
    "\"\"\"\n",
    "for i in range(len(all_variables)):\n",
    "    cont_mat = np.array(all_variables[i], dtype=float)\n",
    "    cont_mat_pad = np.zeros((Max_len_CDR, 1))\n",
    "    cont_mat_pad[:cont_mat.shape[0], 0] = cont_mat\n",
    "    cont_mats.append(cont_mat_pad)\n",
    "\n",
    "lbls = np.stack(cont_mats)\n",
    "lbls.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating input X matrix using Meiler's encoding and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3816, 38, 28)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##cdrs (padding Meiler's encoding of shape)\n",
    "\"\"\"\n",
    "From the CDR dataframe, extract the CDR column, apply one-hot encoding on every residue,\n",
    "pad the rest with Meiler's encoding\n",
    "E.g. - if the length of residue is 10, will get 10 arrays for every residue which will be a combination of both\n",
    "one-hot encoding and Meiler's encoding. So, for every residue we will have (21+7) features(channels).\n",
    "\n",
    "Assume we have Max_len of CDR as 21. We will have to pad the remaining 11 with zeros \n",
    "21 columns as its the max len of the CDR.\n",
    "If we have 1998 such CDRs the shape of out input cdrs matrix will be\n",
    "\n",
    "cdrs.shape = (1998, 21, 28)\n",
    "\"\"\"\n",
    "cdr_loop = pd.read_pickle(\"CDRs_seqno_pdb.pkl\")\n",
    "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
    "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
    "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
    "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
    "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
    "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
    "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
    "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\"])].index)\n",
    "cdr_loop.reset_index(drop=True, inplace=True)\n",
    "\n",
    "NUM_FEATURES = 28\n",
    "cdr_mats = []\n",
    "for i in range(len(cdr_loop['CDR'])):\n",
    "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
    "    cdr_mat = seq_to_one_hot(cdr_loop['CDR'][i])\n",
    "    cdr_mat_pad = np.zeros((Max_len_CDR, NUM_FEATURES))\n",
    "    cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n",
    "    cdr_mats.append(cdr_mat_pad)\n",
    "cdrs = np.stack(cdr_mats)\n",
    "cdrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3816, 38, 1)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##masks (1 for len of chain rest 0)\n",
    "cdr_masks = []\n",
    "for i in range(len(cdr_loop['CDR'])):\n",
    "    cdr_mask = np.zeros((Max_len_CDR, 1), dtype=int)\n",
    "    cdr_mask[:len(cdr_loop['CDR'][i]), 0] = 1\n",
    "    cdr_masks.append(cdr_mask)\n",
    "masks = np.stack(cdr_masks)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cdr_len = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "output_folder = \"Crossvalidation_cut_off_3\"\n",
    "run_cv(output_folder, num_iters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metrics for cut_off = 5A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "cv_result_folder = \"Crossvalidation_cut_off_5\"\n",
    "cv_num_iters = 10\n",
    "\n",
    "for i, loop in enumerate([\"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\"]):\n",
    "    print(\"Classifier metrics for loop type\", loop)\n",
    "    labels, probs = open_crossval_results(cv_result_folder, 10, i)\n",
    "    compute_classifier_metrics(labels, probs)\n",
    "    \n",
    "    print(\"Plotting PR curves\")\n",
    "#     labels, probs = open_crossval_results(cv_result_folder, cv_num_iters)\n",
    "    fig = plot_pr_curve(labels, probs, colours=(\"#0072CF\", \"#68ACE5\"),\n",
    "                        label=\"Parapred\",plot_fig = None)\n",
    "    fig\n",
    "    fig1 = plot_roc_curve(labels, probs, colours=(\"#0072CF\", \"#68ACE5\"),\n",
    "                        label=\"Parapred\",plot_fig = None)\n",
    "    fig1\n",
    "# #     fig = plot_pr_curve(labels_abip, probs_abip, colours=(\"#D6083B\", \"#EB99A9\"),\n",
    "# #                         label=\"Parapred using ABiP data\", plot_fig=fig)\n",
    "# fig = plot_abip_pr(fig)\n",
    "# fig.savefig(\"pr.eps\")\n",
    "\n",
    "print(\"Computing classifier metrics\")\n",
    "# compute_classifier_metrics(labels, probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics for cut_off = 5A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "cv_result_folder = \"Crossvalidation_cut_off_3\"\n",
    "cv_num_iters = 10\n",
    "\n",
    "for i, loop in enumerate([\"H1\", \"H2\", \"H3\", \"L1\", \"L2\", \"L3\"]):\n",
    "    print(\"Classifier metrics for loop type\", loop)\n",
    "    labels, probs = open_crossval_results(cv_result_folder, 10, i)\n",
    "    compute_classifier_metrics(labels, probs)\n",
    "    \n",
    "    print(\"Plotting PR curves\")\n",
    "#     labels, probs = open_crossval_results(cv_result_folder, cv_num_iters)\n",
    "    fig = plot_pr_curve(labels, probs, colours=(\"#0072CF\", \"#68ACE5\"),\n",
    "                        label=\"Parapred\"+loop,plot_fig = None)\n",
    "    fig\n",
    "    fig1 = plot_roc_curve(labels, probs, colours=(\"#0072CF\", \"#68ACE5\"),\n",
    "                        label=\"Parapred\"+loop,plot_fig = None)\n",
    "    fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
